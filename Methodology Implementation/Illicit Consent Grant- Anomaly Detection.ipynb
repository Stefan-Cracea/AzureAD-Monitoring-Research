{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem, t\n",
    "from numpy import mean, std\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from user_agents import parse\n",
    "from tensorflow.keras.losses import mse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random as random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa962157",
   "metadata": {},
   "source": [
    "<b>Feature Engineering - Illicit Consent Grant</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the features \"ConsentCount\" (counts the number of consents in a given day), \n",
    "\"RecentConsentCount\" (counts the number of consents in a 14 day period)\n",
    "\"recentConsent\" (boolean feature stating whether the consent was recend, i.e., less than 14 days).\n",
    "\n",
    "'''\n",
    "\n",
    "def check_recent_consent(act):\n",
    "    if act['Activity'] == \"Consent to application\":\n",
    "        date_add_sp = df_audit[(df_audit['Target1DisplayName'] == act['Target1DisplayName']) & \n",
    "                                (df_audit['Activity'] == \"Add service principal\")]['Date (UTC)']\n",
    "        if (not date_add_sp.empty):\n",
    "            return 0 <= (pd.to_datetime(act['Date (UTC)']) - pd.to_datetime(date_add_sp.iloc[0])).days < 14\n",
    "    return False\n",
    "\n",
    "df_audit['recentConsent'] = df_audit.apply(check_recent_consent, axis=1)\n",
    "\n",
    "############## The number of consents to apps recently added ##################\n",
    "\n",
    "def count_recent_consent(df_audit):\n",
    "    # Filter and group by 'Target1DisplayName' for recent consents\n",
    "    consent_counts = df_audit[(df_audit['Activity'] == 'Consent to application') & (df_audit['recentConsent'] == True)]\\\n",
    "        .groupby('Target1DisplayName').size().reset_index(name='RecentConsentCount')\n",
    "\n",
    "    df_audit = pd.merge(df_audit, consent_counts, on='Target1DisplayName', how='left')\n",
    "\n",
    "    # Fill null entries with 0.\n",
    "    df_audit['RecentConsentCount'] = df_audit['RecentConsentCount'].fillna(0).astype(int)\n",
    "\n",
    "    return df_audit\n",
    "\n",
    "\n",
    "def count_consent(df_audit):\n",
    "    # Bin by \"Day\"\n",
    "    df_audit['Date'] = pd.to_datetime(df_audit['Date (UTC)']).dt.date\n",
    "\n",
    "    # Group by app name ('Target1DisplayName'), day (\"Date\") and activity, and count the number of consents\n",
    "    consent_counts = df_audit[df_audit['Activity'] == 'Consent to application']\\\n",
    "        .groupby(['Target1DisplayName', 'Date', 'Activity']).size().reset_index(name='ConsentCount')\n",
    "\n",
    "    df_audit = pd.merge(df_audit, consent_counts, on=['Target1DisplayName', 'Date', 'Activity'], how='left')\n",
    "\n",
    "    # Fill null entries with 0.\n",
    "    df_audit['ConsentCount'] = df_audit['ConsentCount'].fillna(0).astype(int)\n",
    "    df_audit = df_audit.drop(columns=['Date'])\n",
    "\n",
    "    return df_audit\n",
    "\n",
    "df_audit = count_recent_consent(df_audit)\n",
    "df_audit = count_consent(df_audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd503f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For every \"Add delegated permission grant\" event type, create a feature that contains the consent scope (consentScope)\n",
    "Create a variable with the name riskyPermissionsInScope that represents the number of risky permissions that \n",
    "are in the scope of the consent. Risky permissions are: Mail.* (including Mail.Send*, but not Mail.ReadBasic*), Contacts. *, \n",
    "MailboxSettings.*, People.*, Files.*, Notes.*, Directory.AccessAsUser.All, User_Impersonation. \n",
    "(https://learn.microsoft.com/en-us/security/operations/incident-response-playbook-app-consent)\n",
    "Also check for application permissions, as all of them should be considered high risk.\n",
    "'''\n",
    "df_audit_copy = df_audit.copy()\n",
    "\n",
    "\n",
    "df_audit_copy['delegatedPermissionsScope'] = df_audit_copy.apply(lambda row: re.findall('\"([^\"]*)\"', row['Target1ModifiedProperty1NewValue'])[0] \\\n",
    "                                            if row['Activity'] == 'Add delegated permission grant' \\\n",
    "                                            or row['Activity'] == 'Remove delegated permission grant'\\\n",
    "                                            else 'Unknown', axis=1)\n",
    "\n",
    "df_audit_copy['appPermissionsScope'] = df_audit_copy.apply(lambda row: re.findall('\"([^\"]*)\"', row['Target1ModifiedProperty2NewValue'])[0] \\\n",
    "                                            if row['Activity'] == 'Add app role assignment to service principal' \\\n",
    "                                            else (re.findall('\"([^\"]*)\"', row['Target1ModifiedProperty2NewValue'])[0] \\\n",
    "                                            if row['Activity'] == 'Remove app role assignment to service principal' else 'Unknown'), axis=1)\n",
    "\n",
    "#Create a column that holds the name of the app that requires consent. \n",
    "# Create list with all event types that have the app name in the same variable\n",
    "operation_names = ['Add application', 'Add service principal', 'Update application', \\\n",
    "                   'Update service principal', 'Consent to application', 'Add app role assignment grant to user']\n",
    "\n",
    "df_audit_copy['appName'] = df_audit_copy.apply(lambda row: re.findall('\"([^\"]*)\"', row['Target1ModifiedProperty2NewValue'])[0] \\\n",
    "                                            if row['Activity'] in operation_names\\\n",
    "                                            else 'Unknown', axis=1)\n",
    "\n",
    "#Create a column that shows the onBehalfOfAll value of the \"Consent to application\" operation\n",
    "df_audit_copy['onBehalfOfAll'] = df_audit_copy.apply(lambda row:  re.findall('\"([^\"]*)\"', row['Target1ModifiedProperty3NewValue'])[0] \\\n",
    "                                            if row['Activity'] == 'Consent to application' else 'Unknown', axis=1)\n",
    "\n",
    "\n",
    "#Create a column that shows the isRiskyApp value of the \"Consent to application\" operation\n",
    "df_audit_copy['isRiskyApp'] = False\n",
    "\n",
    "df_audit_copy['isRiskyApp'] =  df_audit_copy.apply(lambda row: True \\\n",
    "                                           if \"Risky application detected\" in str(row['Target1ModifiedProperty4NewValue'])\\\n",
    "                                           and (row['Activity'] == 'Consent to application') else False, axis=1)\n",
    "    \n",
    "# Also create the feature Weekend \n",
    "df_audit_copy['Weekend'] = df_audit_copy['Date (UTC)'].dt.weekday >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CorrelationId', 'Activity', 'Date (UTC)', 'Result', 'ActorUserPrincipalName','delegatedPermissionsScope',\\\n",
    "           'appPermissionsScope', 'isRiskyApp', 'ConsentCount', 'RecentConsentCount', 'recentConsent',\\\n",
    "           'onBehalfOfAll', 'consentType', 'isAppOnly', 'appName', 'simulatedAttack', 'Label_Role_Management', 'Label_o365']\n",
    "\n",
    "df_audit_selected_events = df_audit_copy[features].copy()\n",
    "\n",
    "def propagate_values(group):\n",
    "    mask_consent = group['Activity'] == 'Consent to application'\n",
    "    mask_delegated = (group['Activity'] == 'Remove delegated permission grant') | \n",
    "    (group['Activity'] == 'Add delegated permission grant') \n",
    "    mask_app_role = (group['Activity'] == 'Remove app role assignment to service principal') | \n",
    "    (group['Activity'] == 'Add app role assignment to service principal')\n",
    "    \n",
    "    # Create a grouping using the masks for defined above\n",
    "    \n",
    "    if mask_1.any():\n",
    "        values_row = grouping.loc[mask_consent].iloc[0]\n",
    "        grouping['onBehalfOfAll'] = values_row['onBehalfOfAll']\n",
    "        grouping['appName'] = values_row['appName']\n",
    "    if mask_2.any():\n",
    "        values_row = grouping.loc[mask_delegated].iloc[0]\n",
    "        grouping['consentType'] = values_row['consentType']\n",
    "    if mask_3.any():\n",
    "        values_row = grouping.loc[mask_app_role].iloc[0]\n",
    "        grouping['consentType'] = values_row['consentType']\n",
    "    return group\n",
    "\n",
    "# Group by correlation ID and apply the function to each grouping\n",
    "df_audit_selected_events = df_audit_selected_events.groupby('CorrelationId', group_keys=False).apply(propagate_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create features \"numberDelegatedRiskyPermissionsPerApp\", \"numberDelegatedNonRiskyPermissionsPerApp\" that track \n",
    "the number of risky/non-risky delegated permissions an app currently has in scope.\n",
    "Define a list of risky permissions according to Microsoft.\n",
    "(https://learn.microsoft.com/en-us/security/operations/incident-response-playbook-app-consent)\n",
    "'''\n",
    "\n",
    "#This list represents risky permissions from both delegated and AppRole side.\n",
    "risky_permissions_del_app = ['Mail.', 'Contacts.', 'MailboxSettings.', 'People.', 'Files.', 'Notes.', 'Directory.AccessAsUser.All',\\\n",
    "                     'User_Impersonation', 'Application.ReadWrite.All', 'Directory.ReadWrite.All', 'Domain.ReadWrite.All',\\\n",
    "                    'EduRoster.ReadWrite.All', 'Group.ReadWrite.All', 'Member.Read.Hidden', 'RoleManagement.ReadWrite.Directory',\\\n",
    "                    'User.ReadWrite.All', 'User.ManageIdentities.All']\n",
    "\n",
    "#However, as app permissions are more dangerous, we shall consider also every permission with write access as being risky.\n",
    "risky_write_app = 'Write'\n",
    "\n",
    "# Keeping Track of risky/non-risky delegated permissions for each Application\n",
    "risky_permissions = {}\n",
    "non_risky_permissions = {}\n",
    "\n",
    "# Function used for counting the number of delegated risky permissions\n",
    "def count_delegated_risky_perms(act):\n",
    "    corrID = act['CorrelationId']\n",
    "    \n",
    "    # Add the correlation ID to the dictionary if not present already\n",
    "    if corrID not in risky_permissions:\n",
    "        risky_permissions[corrID] = 0\n",
    "        \n",
    "    # Access only the activities related to the delegated permission scope of a consent grant\n",
    "    if act['Activity'] == 'Add delegated permission grant' or act['Activity'] == 'Remove delegated permission grant':\n",
    "        delegated_perms_scope = act['delegatedPermissionsScope'].split(\" \")\n",
    "        if risky_permissions[corrID] == 0:\n",
    "            for perm in delegated_perms_scope:\n",
    "                for risky_perms in risky_permissions_del_app:\n",
    "                    if risky_perms in perm and risky_perms != \"Mail.ReadBasic\":\n",
    "                        risky_permissions[corrID] += 1\n",
    "            if ('offline_access' in delegated_perms_scope) & (risky_permissions[corrID] != 0):\n",
    "                risky_permissions[corrID] += 1\n",
    "    return risky_permissions[corrID]\n",
    "\n",
    "# Function used for counting the number of delegated non-risky permissions\n",
    "def count_delegated_non_risky_perms(act):\n",
    "    corrID = act['CorrelationId']\n",
    "    \n",
    "    # Add the correlation ID to the dictionary if not present already\n",
    "    if corrID not in non_risky_permissions:\n",
    "        non_risky_permissions[corrID] = 0\n",
    "    if act['Activity'] == 'Add delegated permission grant' or act['Activity'] == 'Remove delegated permission grant':\n",
    "        if '' in act['delegatedPermissionsScope'].split(\" \"):\n",
    "            non_risky_permissions[corrID] = len(act['delegatedPermissionsScope'].split(\" \")) - 1 - act['numberDelegatedRiskyPermissionsPerApp']\n",
    "        else:\n",
    "            non_risky_permissions[corrID] = len(act['delegatedPermissionsScope'].split(\" \")) - act['numberDelegatedRiskyPermissionsPerApp']\n",
    "    return non_risky_permissions[corrID]\n",
    "\n",
    "# Create new features\n",
    "df_audit_selected_events['numberDelegatedRiskyPermissionsPerApp'] = df_audit_selected_events.apply(count_delegated_risky_perms, axis=1)\n",
    "df_audit_selected_events['numberDelegatedNonRiskyPermissionsPerApp'] = df_audit_selected_events.apply(count_delegated_non_risky_perms, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create features \"numberAppRoleRiskyPermissionsPerApp\", \"numberAppRoleNonRiskyPermissionsPerApp\" that track \n",
    "the number of risky/non-risky app role permissions an app currently has in scope.\n",
    "Define a list of risky permissions according to Microsoft.\n",
    "(https://learn.microsoft.com/en-us/security/operations/incident-response-playbook-app-consent)\n",
    "'''\n",
    "\n",
    "# Keeping Track of risky/non-risky app role permissions for each Application\n",
    "app_risky_permissions = {}\n",
    "app_non_risky_permissions = {}\n",
    "\n",
    "# Check each row in the dataset for app role permissions\n",
    "for index, row in df_audit_selected_events.iterrows():\n",
    "    ok = 0\n",
    "    # get the app name and risky permissions list for this row\n",
    "    app_name = row['appName']\n",
    "    app_risky_permissions = row['appPermissionsScope']\n",
    "    \n",
    "    # check if app_name is already in the dictionary\n",
    "    if app_name not in num_app_risky_perms_dict:\n",
    "        num_app_risky_perms_dict[app_name] = 0\n",
    "    if app_name not in num_app_non_risky_perms_dict:\n",
    "        num_app_non_risky_perms_dict[app_name] = 0\n",
    "\n",
    "    # update count based on operationName\n",
    "    for perm in risky_permissions_del_app:\n",
    "        if perm in app_risky_permissions or risky_write_app in app_risky_permissions:\n",
    "            if row['Activity'] == 'Add app role assignment to service principal':\n",
    "                num_app_risky_perms_dict[app_name] += 1\n",
    "            elif row['Activity'] == 'Remove app role assignment to service principal':\n",
    "                num_app_risky_perms_dict[app_name] -= 1\n",
    "            ok = 1\n",
    "            break\n",
    "        else:\n",
    "            if row['Activity'] == 'Add app role assignment to service principal':\n",
    "                num_app_non_risky_perms_dict[app_name] += 1\n",
    "            elif row['Activity'] == 'Remove app role assignment to service principal':\n",
    "                num_app_non_risky_perms_dict[app_name] -= 1\n",
    "            ok = 2\n",
    "            break\n",
    "    \n",
    "    # assign the count to the row in the dataframe\n",
    "    df_audit_selected_events.loc[index, 'numberAppRoleRiskyPermissionsPerApp'] = num_app_risky_perms_dict[app_name]\n",
    "    df_audit_selected_events.loc[index, 'numberAppRoleNonRiskyPermissionsPerApp'] = num_app_non_risky_perms_dict[app_name]\n",
    "\n",
    "# The features required for the anomaly detection task.\n",
    "# Note that \"onBehalfOfAll\" is the same as \"ConsentType\"\n",
    "features = ['ActorUserPrincipalName','isRiskyApp','numberAppRoleRiskyPermissionsPerApp','numberAppRoleNonRiskyPermissionsPerApp', \n",
    "              'numberDelegatedRiskyPermissionsPerApp', 'numberDelegatedNonRiskyPermissionsPerApp', 'Weekend', 'ConsentCount', \n",
    "            'RecentConsentCount', 'recentConsent', 'onBehalfOfAll', 'simulatedAttack', 'Label_Role_Management', 'Label_o365']\n",
    "    \n",
    "df_audit_flows = df_audit_selected_events[df_audit_selected_events['Activity'] == 'Consent to application']\n",
    "df_audit_flows = df_audit_flows.reset_index(drop=True)\n",
    "df_audit_flows = df_audit_flows[features]\n",
    "\n",
    "# Ground truth labeling\n",
    "df_audit_flow.loc[df_audit_flow['simulatedAttack'] != 'Normal', 'Attack'] = 1\n",
    "df_audit_flow.loc[df_audit_flow['simulatedAttack'] == 'Normal', 'Attack'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa173e64",
   "metadata": {},
   "source": [
    "<b>Synthetic Data Generation - Illicit Consent Grant</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(data):\n",
    "    # Extracting the probability distribution for each feature\n",
    "    probabilities = {}\n",
    "    for feature in data.columns:\n",
    "        values = data[feature].value_counts(normalize=True).index.tolist()\n",
    "        prob = data[feature].value_counts(normalize=True).values.tolist()\n",
    "        probabilities[feature] = (values, prob)\n",
    "    \n",
    "    # Estimating joint probabilities\n",
    "    joint_probs = real_data.groupby(['RecentConsentCount', \\\n",
    "                                     'recentConsent']).size() / len(real_data)\n",
    "    probs['joint_consent_count'] = joint_probs\n",
    "    return probabilities\n",
    "\n",
    "def generate_synthetic_data(n_samples):\n",
    "    data = {\n",
    "        'ActorUserPrincipalName': np.random.choice(probabilities['ActorUserPrincipalName'][0], n_samples, p=probabilities['ActorUserPrincipalName'][1]),\n",
    "        'isRiskyApp': [False] * n_samples,\n",
    "        'numberAppRoleRiskyPermissionsPerApp': np.random.choice([0, 1], n_samples),\n",
    "        'numberAppRoleNonRiskyPermissionsPerApp': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "        'numberDelegatedRiskyPermissionsPerApp': np.random.choice([0, 1, 2, 3, 4, 5], n_samples),\n",
    "        'numberDelegatedNonRiskyPermissionsPerApp': np.random.choice([0, 1, 2, 3, 4, 5, 6], n_samples),\n",
    "        'Weekend': [False] * n_samples,\n",
    "        'ConsentCount': [1] * n_samples,\n",
    "#         'RecentConsentCount': np.random.choice(probabilities['RecentConsentCount'][0], n_samples, p=probabilities['RecentConsentCount'][1]),\n",
    "#         'recentConsent': np.random.choice(probabilities['recentConsent'][0], n_samples, p=probabilities['recentConsent'][1]),\n",
    "        'onBehalfOfAll': np.random.choice(probabilities['onBehalfOfAll'][0], n_samples, p=probabilities['onBehalfOfAll'][1]),\n",
    "    }\n",
    "    \n",
    "    # Handling joint probabilities\n",
    "    recentconsent_choices, recentconsent_probs = list(probs['joint_consent_count'].index), probs['joint_consent_count'].values\n",
    "    recentconsent_samples = np.random.choice(range(len(recentconsent_choices)), p=recentconsent_probs, size=n_samples)\n",
    "    data['RecentConsentCount'], data['recentConsent'] = zip(*[recentconsent_choices[i] for i in recentconsent_samples])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Split into legitimate and attack df\n",
    "df_audit_legitimate = df_audit_flow[df_audit_flow['simulatedAttack'] != 'Normal'].copy()\n",
    "df_audit_attack = df_audit_flow[df_audit_flow['simulatedAttack'] == 'Normal'].copy()\n",
    "\n",
    "probabilities = estimate_probabilities(df_audit_legitimate)\n",
    "synthetic_df = generate_synthetic_samples(1000)\n",
    "\n",
    "# Create split proportions\n",
    "train_size = int(n * 0.7)\n",
    "threshold_size = int(n * 0.1)\n",
    "\n",
    "# Split the dataset\n",
    "train_data = synthetic_data_training[:train_size]\n",
    "threshold_data = synthetic_data_training[train_size:train_size+threshold_size]\n",
    "validation_data = synthetic_data_training[train_size+threshold_size:train_size+2*threshold_size]\n",
    "test_data = synthetic_data_training[train_size+2*threshold_size:]\n",
    "\n",
    "# Inject the attack data in the validation and test splits\n",
    "validation_data_copy = validation_data.copy()\n",
    "\n",
    "# Select a random position to add the attacks to\n",
    "n = random.randint(1, 40)\n",
    "validation_data_add_before = validation_data_copy.iloc[:n]\n",
    "validation_data_add_after = validation_data_copy.iloc[n:]\n",
    "\n",
    "validation_data_copy = pd.concat([validation_data_add_before, \\\n",
    "                       df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackAdminVictim'][engineered_features_3], \\\n",
    "                       validation_data_add_after], ignore_index=True)\n",
    "\n",
    "n = random.randint(40, 70)\n",
    "validation_data_add_before = validation_data_copy.iloc[:n]\n",
    "validation_data_add_after = validation_data_copy.iloc[n:]\n",
    "validation_data_copy = pd.concat([validation_data_add_before, \\\n",
    "               df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackUserVictims1'][engineered_features_3], \\\n",
    "               validation_data_add_after], ignore_index=True)\n",
    "\n",
    "n = random.randint(70, 100)\n",
    "validation_data_add_before = validation_data_copy.iloc[:n]\n",
    "validation_data_add_after = validation_data_copy.iloc[n:]\n",
    "validation_data_copy = pd.concat([val_data_add_before_third_attack, \\\n",
    "                df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackUserVictims2'][engineered_features_3], \\\n",
    "                val_data_add_after_third_attack], ignore_index=True)\n",
    "\n",
    "validation_data_copy[validation_data_copy['Attack'] == 1]\n",
    "\n",
    "n = random.randint(1, 40)\n",
    "test_data_add_before = df_test.iloc[:n]\n",
    "test_data_add_after = df_test.iloc[n:]\n",
    "\n",
    "test_data_copy = pd.concat([test_data_add_before, \\\n",
    "                           df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackAdminVictim2'], \\\n",
    "                           test_data_add_after], ignore_index=True)\n",
    "n = random.randint(40, 70)\n",
    "test_data_add_before = test_data_copy.iloc[:n]\n",
    "test_data_add_after = test_data_copy.iloc[n:]\n",
    "test_data_copy = pd.concat([test_data_add_before, \\\n",
    "                           df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackUserVictims3'], \\\n",
    "                           test_data_add_after], ignore_index=True)\n",
    "n = random.randint(70, 100)\n",
    "test_data_add_before = test_data_copy.iloc[:n]\n",
    "test_data_add_after = test_data_copy.iloc[n:]\n",
    "test_data_copy = pd.concat([test_data_add_before, \\\n",
    "                           df_audit_attack[df_audit_attack['simulatedAttack'] == 'AttackUserVictims4'], \\\n",
    "                           test_data_add_after], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ac85c",
   "metadata": {},
   "source": [
    "<b>Add noise to the synthetic data for each split</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fe529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_dataframe(df):\n",
    "    \n",
    "    upn_noise = ['account1@domain.com','account2@domain.com','account3@domain.com', 'account4@domain.com', 'account5@domain.com']\n",
    "\n",
    "    permission_columns = [\n",
    "        \"numberAppRoleRiskyPermissionsPerApp\",\n",
    "        \"numberAppRoleNonRiskyPermissionsPerApp\",\n",
    "        \"numberDelegatedRiskyPermissionsPerApp\",\n",
    "        \"numberDelegatedNonRiskyPermissionsPerApp\"\n",
    "    ]\n",
    "\n",
    "    for col in permission_columns:\n",
    "        noise = np.random.normal(loc=0, scale=0.5, size=len(df))\n",
    "        discrete_noise = np.round(noise)\n",
    "        df[col] = df[col] + discrete_noise\n",
    "        # Ensure the noisy data has valid values\n",
    "        df_training_noisy[col] = np.clip(df_training_noisy[col], 0, df_training_noisy[col].max())\n",
    "    \n",
    "    # Select a few rows at random to add random noise to\n",
    "    noise_probability = 0.02\n",
    "    selected_rows = np.random.rand(len(df)) < noise_probability\n",
    "    \n",
    "    for idx in df[selected_rows].index:\n",
    "        df.at[idx, 'ConsentCount'] += 1\n",
    "        \n",
    "        noisy_row = df.loc[idx].copy()\n",
    "        new_row = noisy_row.copy()\n",
    "        \n",
    "        possible_names = df.loc[df['ActorUserPrincipalName'] != noisy_row['ActorUserPrincipalName'], 'ActorUserPrincipalName']\n",
    "        new_name = np.random.choice(possible_names)\n",
    "        new_row['ActorUserPrincipalName'] = new_name\n",
    "        \n",
    "        top = df.iloc[:idx+1, :]\n",
    "        bottom = df.iloc[idx+1:, :]\n",
    "        df = pd.concat([top, pd.DataFrame(new_row).T, bottom], ignore_index=True)\n",
    "        \n",
    "        to_remove = df[df['ActorUserPrincipalName'] == new_name].sample(1).index\n",
    "        df = df.drop(to_remove)\n",
    "\n",
    "    df.loc[(df['ConsentCount'] == 2) & (df['RecentConsentCount'] == 1), 'RecentConsentCount'] = 2\n",
    "\n",
    "    noise = np.random.normal(loc=0, scale=1, size=len(df))\n",
    "    replace_mask = np.abs(noise) > 2\n",
    "    df.loc[replace_mask, \"ActorUserPrincipalName\"] = np.random.choice(upn_noise, size=replace_mask.sum())\n",
    "\n",
    "    return df\n",
    "\n",
    "df_training =  train_data\n",
    "df_test =  test_data_copy\n",
    "df_validation = validation_data_copy\n",
    "df_threshold = threshold_data\n",
    "\n",
    "df_training = add_noise_to_dataframe(df_training)\n",
    "df_threshold = add_noise_to_dataframe(df_threshold)\n",
    "\n",
    "subset0 = df_validation[df_validation_noisy['Attack'] == 0].copy()\n",
    "subset1 = df_validation[df_validation_noisy['Attack'] == 1].copy()\n",
    "subset2 = df_test[df_test_noisy['Attack'] == 0].copy()\n",
    "subset3 = df_test[df_test_noisy['Attack'] == 1].copy()\n",
    "\n",
    "subset_0 = add_noise_to_dataframe(subset_0)\n",
    "subset_2 = add_noise_to_dataframe(subset_2)\n",
    "\n",
    "df_validation = pd.concat([subset_0, subset_1]).sort_index()\n",
    "df_test = pd.concat([subset_2, subset_3]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c04c58",
   "metadata": {},
   "source": [
    "<b>Data Transformation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040da847",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_list = ['ActorUserPrincipalName','isRiskyApp', 'Weekend', 'recentConsent','onBehalfOfAll']\n",
    "numerical_list = ['RecentConsentCount', 'ConsentCount', 'numberDelegatedNonRiskyPermissionsPerApp', \\\n",
    "                 'numberDelegatedRiskyPermissionsPerApp', 'numberAppRoleNonRiskyPermissionsPerApp', 'numberAppRoleRiskyPermissionsPerApp']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # create a MinMaxScaler object\n",
    "scaler.fit(df_training[numerical_list])  # fit the scaler on the training dataset\n",
    "\n",
    "# normalize each column not in drop_list based on the min-max from the training dataset\n",
    "df_training[numerical_list] = scaler.transform(df_training[numerical_list])\n",
    "\n",
    "\n",
    "df_threshold[numerical_list] = scaler.transform(df_threshold[numerical_list])\n",
    "df_validation[numerical_list] = scaler.transform(df_validation[numerical_list])\n",
    "df_test[numerical_list] = scaler.transform(df_test[numerical_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df384fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the binary categorical variables (as 0 or 1)\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_list:\n",
    "    if col != \"ActorUserPrincipalName\":\n",
    "        \n",
    "        # Create a mapping for known labels\n",
    "        label_mapping = {label: idx for idx, label in enumerate(df_train[col].unique())}\n",
    "        \n",
    "        # Ensure we handle both True and False in the mapping\n",
    "        if True not in label_mapping:\n",
    "            label_mapping[True] = 1\n",
    "        if False not in label_mapping:\n",
    "            label_mapping[False] = 0\n",
    "        \n",
    "        # Apply the mapping to the datasets\n",
    "        df_training[col] = df_train[col].map(label_mapping)\n",
    "        df_threshold[col] = df_threshold[col].map(label_mapping)\n",
    "        df_validation[col] = df_validation[col].map(label_mapping)\n",
    "        df_test[col] = df_test[col].map(label_mapping)\n",
    "        \n",
    "        label_encoders[col] = label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"Unseen\" category\n",
    "df_train[\"ActorUserPrincipalName\"].astype('str').astype('category').cat.add_categories([\"Unseen\"], inplace=True)\n",
    "\n",
    "# Convert values that are not in training to \"Unseen\"\n",
    "df_validation.loc[~df_validation[\"ActorUserPrincipalName\"].isin(df_train[\"ActorUserPrincipalName\"]), \"ActorUserPrincipalName\"] = \"Unseen\"\n",
    "df_test.loc[~df_test[\"ActorUserPrincipalName\"].isin(df_train[\"ActorUserPrincipalName\"]), \"ActorUserPrincipalName\"] = \"Unseen\"\n",
    "\n",
    "# one-hot encode the \"ActorUserPrincipalName\" feature in each split\n",
    "df_train = pd.get_dummies(df_train, columns=[\"ActorUserPrincipalName\"])\n",
    "df_threshold = pd.get_dummies(df_threshold, columns=[\"ActorUserPrincipalName\"])\n",
    "df_validation = pd.get_dummies(df_validation, columns=[\"ActorUserPrincipalName\"])\n",
    "df_test = pd.get_dummies(df_test, columns=[\"ActorUserPrincipalName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcf170",
   "metadata": {},
   "source": [
    "<b><font size=\"3\">Illicit Consent Grant: Modeling Phase</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_only = [i for i in df_train if 'ActorUserPrincipalName_' in i]\n",
    "permissions_only = ['numberDelegatedNonRiskyPermissionsPerApp', 'numberDelegatedNonRiskyPermissionsPerApp',\\\n",
    "                    'numberAppRoleNonRiskyPermissionsPerApp', 'numberAppRoleRiskyPermissionsPerApp']\n",
    "recent_consent = ['RecentConsentCount', 'ConsentCount', 'recentConsent']\n",
    "\n",
    "recent_consent_and_permissions = recent_consent + permissions_only\n",
    "recent_consent_and_permissions_and_users = recent_consent_and_permissions + users_only\n",
    "permissions_and_users = permissions_only + users_only\n",
    "\n",
    "features_dict = {\n",
    "    \"users_only\": users_only,\n",
    "    \"all_features\": all_features,\n",
    "    \"recent_consent_and_permissions_and_users\": recent_consent_and_permissions_and_users,\n",
    "    \"recent_consent_and_permissions\": recent_consent_and_permissions,\n",
    "    \"permissions_only\": permissions_only,\n",
    "    \"recent_consent\": recent_consent,\n",
    "    \"permissions_and_users\": permissions_and_users\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5188d6",
   "metadata": {},
   "source": [
    "<b>Modeling Autoencoder</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Autoencoder Model initialization.\n",
    "'''\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Function creating the Autoencoder network\n",
    "def build_autoencoder(drop_r, l2_reg, l_rate, input_dim):\n",
    "    \n",
    "    # Other hyperparameters about the network structure of the AE (chosen empirically)\n",
    "    bottleneck_size = 2        # Controls the size of the encoding\n",
    "    num_hidden_layers = 2      # Controls the number of hidden layers\n",
    "    ratio = 0.5                # Controls the number of neurons in each layer\n",
    "    \n",
    "    # Input layer initialization\n",
    "    layer_dims = [input_dim]\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Dense layer with sigmoid activation function\n",
    "    x = layers.Dense(input_dim, activation=\"sigmoid\")(input_layer)\n",
    "\n",
    "    for i in range(num_hidden_layers//2):\n",
    "        new_dim = int(layer_dims[-1] * 0.5)\n",
    "        layer_dims.append(new_dim)\n",
    "    \n",
    "    # Encoding layers\n",
    "    encoder = tf.keras.Sequential([\n",
    "      layers.Dropout(drop_r),\n",
    "      layers.Dense(layer_dims[-1], activation='relu', kernel_regularizer=l2_reg),\n",
    "      layers.Dropout(drop_r),\n",
    "      layers.Dense(bottleneck_size, activation=activation_func, kernel_regularizer=l2_reg)])(x)\n",
    "    \n",
    "    # Decoding layers\n",
    "    decoder = tf.keras.Sequential([\n",
    "          layers.Dense(layer_dims[-1], activation='relu', kernel_regularizer=l2_reg),\n",
    "          layers.Dense(input_dim, activation=\"sigmoid\")])(encoder)\n",
    "    \n",
    "    # Create the autoencoder\n",
    "    autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "    opt = Adam(learning_rate=l_rate)\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computing the CI\n",
    "confidence_level = 0.95 \n",
    "degrees_freedom = 49\n",
    "\n",
    "# Hyperparameter grid space\n",
    "dropouts = [0.3, 0.5]\n",
    "l2_regs = [1e-6, 5e-6, 1e-5, 5e-5]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "\n",
    "all_configs = list(itertools.product(dropouts, l2_regs, learning_rates, bottleneck_sizes))\n",
    "\n",
    "# Set the number of iterations for random search\n",
    "n_iterations = 100\n",
    "\n",
    "# Other hyperparameters chosen empirically\n",
    "epochs_num = 20\n",
    "selected_batch_size = 64\n",
    "\n",
    "\n",
    "for feature_name, feature in features_dict.items():\n",
    "    print(feature_name)\n",
    "    X_train = df_train[feature].copy()\n",
    "    X_validation = df_validation[feature].copy()\n",
    "    y_validation = df_validation['Attack']\n",
    "    X_threshold = df_threshold[feature].copy()\n",
    "    \n",
    "    all_results = []\n",
    "    input_dim = X_train.shape[1]\n",
    "    for thrsh in range(4)\n",
    "        for config in all_configs:\n",
    "            f1_scores, precisions, recalls, fprs = [], [], [], []\n",
    "\n",
    "            for iteration in range(n_iter):\n",
    "\n",
    "\n",
    "                dr = config[0]\n",
    "                l2_r = l2(config[1])\n",
    "                l_r = config[2]\n",
    "\n",
    "                # Build and train the model\n",
    "                model = build_autoencoder(dr, l2_r, l_r, input_dimension)\n",
    "                model.fit(X_train, X_train, shuffle=False, epochs=epochs_num, batch_size=selected_batch_size, verbose=0)\n",
    "\n",
    "                # Compute the reconstruction error of the threshold set\n",
    "                X_threshold_predicted = model.predict(X_threshold)\n",
    "                re_threshold = np.mean(np.power(X_threshold - X_threshold_predicted, 2), axis=1)\n",
    "\n",
    "                if thrsh == 0:\n",
    "                    #MAD\n",
    "                    median_reconstruction_errors = np.median(re_threshold)\n",
    "                    mad = 1.4826 * np.median(np.abs(re_threshold - median_reconstruction_errors))\n",
    "                    threshold = median_reconstruction_errors + 3 * mad\n",
    "                    threshold_name = 'MAD'\n",
    "                elif thrsh == 1:\n",
    "                    #IQR\n",
    "                    Q1 = np.percentile(re_threshold, 25)\n",
    "                    Q3 = np.percentile(re_threshold, 75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    threshold = Q3 + 1.5*IQR\n",
    "                    threshold_name = 'IQR'\n",
    "                elif thrsh == 2:\n",
    "                    # 2 sigma\n",
    "                    threshold = np.mean(re_threshold) + 2 * np.std(re_threshold)\n",
    "                    threshold_name = '2-sigma'\n",
    "                else:\n",
    "                    # 3 sigma\n",
    "                    threshold = np.mean(re_threshold) + 3 * np.std(re_threshold)\n",
    "                    threshold_name = '3-sigma'\n",
    "\n",
    "                # Validation\n",
    "                X_validation_predicted = model.predict(X_validation)\n",
    "                re_validation = np.mean(np.power(X_validation - X_validation_predicted, 2), axis=1)\n",
    "\n",
    "                # Assign an anomaly label to each sample in the validation set based on the computed threshold value\n",
    "                y_predicted_validation = [1 if re > threshold else 0 for re in re_validation]\n",
    "\n",
    "                # Compute F1, precision, recall and fpr\n",
    "                tn, fp, fn, tp = confusion_matrix(y_validation, y_predicted_validation).ravel()\n",
    "                f1 = f1_score(y_validation, y_predicted_validation)\n",
    "                precision = precision_score(y_validation, y_predicted_validation, zero_division=1)\n",
    "                recall = recall_score(y_validation, y_predicted_validation)\n",
    "                fpr = fp / (tn + fp)\n",
    "\n",
    "                # Add the computed metric to their corresponding list\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                fprs.append(fpr)\n",
    "\n",
    "                print(f\"Iteration {iteration + 1}. F1 score: {f1}\")\n",
    "\n",
    "            # After n_iterations, compute the average and the CI\n",
    "\n",
    "            # Average for each metric\n",
    "            avg_f1 = mean(f1_scores)\n",
    "            avg_precision = mean(precisions)\n",
    "            avg_recall = mean(recalls)\n",
    "            avg_fpr = mean(fprs)\n",
    "\n",
    "            # Calculate the standard error of the mean\n",
    "            error_f1 = sem(f1_scores)\n",
    "            error_precision = sem(precisions)\n",
    "            error_recall = sem(recalls)\n",
    "            error_fpr = sem(fprs)\n",
    "\n",
    "            # Calculate the confidence intervals\n",
    "            t_ci_f1 = t.interval(confidence_level, degrees_freedom, avg_f1, error_f1)\n",
    "            t_ci_precision = t.interval(confidence_level, degrees_freedom, avg_precision, error_precision)\n",
    "            t_ci_recall = t.interval(confidence_level, degrees_freedom, avg_recall, error_recall)\n",
    "            t_ci_fpr = t.interval(confidence_level, degrees_freedom, avg_fpr, error_fpr)\n",
    "\n",
    "            print(f\"Average F1 Score: {avg_f1}, Confidence Interval t-Dist: {t_ci_f1}.\")\n",
    "            print(f\"Average Precision: {avg_precision}, Confidence Interval t-Dist: {t_ci_precision}.\")\n",
    "            print(f\"Average Recall: {avg_recall}, Confidence Interval t-Dist: {t_ci_recall}.\")\n",
    "            print(f\"Average FPR: {avg_fpr}, Confidence Interval t-Dist: {t_ci_fpr}.\")\n",
    "\n",
    "            # Create dictionary with the results for each config\n",
    "            config_result = {\n",
    "                    'dropout': dr,\n",
    "                    'l2_regularization': config[1],\n",
    "                    'learning_rate': l_r,\n",
    "                    'threshold': threshold_name,\n",
    "                    'avg_f1': avg_f1,\n",
    "                    'avg_precision': avg_precision,\n",
    "                    'avg_recall': avg_recall,\n",
    "                    'avg_fpr': avg_fpr,\n",
    "                    't_ci_f1': t_ci_f1,\n",
    "                    't_ci_precision': t_ci_precision,\n",
    "                    't_ci_recall': t_ci_recall,\n",
    "                    't_ci_fpr': t_ci_fpr\n",
    "                }\n",
    "\n",
    "            all_results.append(config_result)\n",
    "        \n",
    "        all_results = sorted(all_results, key=lambda x: x['avg_f1'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f358c92",
   "metadata": {},
   "source": [
    "<b>Modeling Isolation Forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a03d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid space\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_samples': [16, 32, 64, 128, 256],\n",
    "    'max_features': [0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    'contamination': [0.055, 0.06, 0.065]\n",
    "}\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# Set the number of iterations for random search\n",
    "n_iterations = 100\n",
    "  \n",
    "# Iterate through all of the feature sets\n",
    "for set_name, feature_set in features_dictionary.items(): \n",
    "    all_configs = list(ParameterGrid(parameter_grid))\n",
    "    all_results = []\n",
    "\n",
    "    X_train = df_training_copy[feature_set].copy()\n",
    "    X_validation = df_encoded_validation_sim_copy[feature_set].copy()\n",
    "    y_validation = df_encoded_validation_sim_copy['Attack']\n",
    "\n",
    "    # List containing the results of the experiments for one feature set in this list\n",
    "    all_results = []\n",
    "    \n",
    "    for config in all_configs:\n",
    "        f1_scores, precisions, recalls, fpr_scores, auc_scores = [], [], [], [], []\n",
    "    \n",
    "        for iteration in range(n_iterations):\n",
    "            model = IsolationForest(**config, bootstrap=True, random_state=rng, n_jobs=-2)\n",
    "            model.fit(X_train)\n",
    "\n",
    "            # Isolation Forest anomaly labels\n",
    "            y_predicted_validation = model.predict(X_validation)\n",
    "            \n",
    "            # Transform the predictions from -1 to 1 and from 1 to 0\n",
    "            y_predicted_validation = (y_predicted_validation == -1).astype(int)\n",
    "            \n",
    "            # Compute F1, precision, recall and fpr\n",
    "            f1 = f1_score(y_validation, y_predicted_validation)\n",
    "            precision = precision_score(y_validation, y_predicted_validation, zero_division=1)\n",
    "            recall = recall_score(y_validation, y_predicted_validation)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_validation, y_predicted_validation).ravel()\n",
    "            fpr = fp / (tn + fp)\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            fpr_scores.append(fpr)\n",
    "\n",
    "            print(f\"Iteration {iteration + 1}. F1 score: {f1}\")\n",
    "\n",
    "        # After n_iterations, compute the average and the CI\n",
    "        \n",
    "        # Average for each metric\n",
    "        avg_f1 = mean(f1_scores)\n",
    "        avg_precision = mean(precisions)\n",
    "        avg_recall = mean(recalls)\n",
    "        avg_fpr = mean(fprs)\n",
    "\n",
    "        # Calculate the standard error of the mean\n",
    "        error_f1 = sem(f1_scores)\n",
    "        error_precision = sem(precisions)\n",
    "        error_recall = sem(recalls)\n",
    "        error_fpr = sem(fprs)\n",
    "\n",
    "        # Calculate the confidence intervals\n",
    "        t_ci_f1 = t.interval(confidence_level, degrees_freedom, avg_f1, error_f1)\n",
    "        t_ci_precision = t.interval(confidence_level, degrees_freedom, avg_precision, error_precision)\n",
    "        t_ci_recall = t.interval(confidence_level, degrees_freedom, avg_recall, error_recall)\n",
    "        t_ci_fpr = t.interval(confidence_level, degrees_freedom, avg_fpr, error_fpr)\n",
    "\n",
    "        print(f\"Average F1 Score: {avg_f1}, Confidence Interval t-Dist: {t_ci_f1}.\")\n",
    "        print(f\"Average Precision: {avg_precision}, Confidence Interval t-Dist: {t_ci_precision}.\")\n",
    "        print(f\"Average Recall: {avg_recall}, Confidence Interval t-Dist: {t_ci_recall}.\")\n",
    "        print(f\"Average FPR: {avg_fpr}, Confidence Interval t-Dist: {t_ci_fpr}.\")\n",
    "        \n",
    "        # Create dictionary with the results for each config\n",
    "        config_result = {\n",
    "            **config,\n",
    "            'avg_f1': avg_f1,\n",
    "            'avg_precision': avg_precision,\n",
    "            'avg_recall': avg_recall,\n",
    "            'avg_fpr': avg_fpr,\n",
    "            't_ci_f1': t_ci_f1,\n",
    "            't_ci_precision': t_ci_precision,\n",
    "            't_ci_recall': t_ci_recall,\n",
    "            't_ci_fpr': t_ci_fpr\n",
    "        }\n",
    "    \n",
    "        all_results.append(config_result)\n",
    "        \n",
    "    all_results = sorted(all_results, key=lambda x: x['avg_f1'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a53a3",
   "metadata": {},
   "source": [
    "<b><font size=\"3\">Password Spray: Evaluation Phase</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d7efe",
   "metadata": {},
   "source": [
    "<b>Evaluation Autoencoder</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameter configuration\n",
    "dr = 0.5\n",
    "l2_r = l2(5e-6)\n",
    "l_r = 1e-4\n",
    "\n",
    "# Best feature set\n",
    "feature_set = recent_consent_and_permissions_and_users\n",
    "\n",
    "X_train = df_training_copy[feature_set].copy()\n",
    "X_threshold = df_threshold_copy[feature_set].copy()\n",
    "X_test = df_test_copy[feature_set].copy()\n",
    "y_test = df_test_copy['Attack']\n",
    "\n",
    "input_dimension = X_train.shape[1]\n",
    "\n",
    "# Build and train the model\n",
    "model = build_autoencoder(dr, l2_r, l_r, btln, input_dimension)\n",
    "history = model.fit(X_train, X_train, shuffle=False, epochs=epochs_num, batch_size=selected_batch_size, verbose=0)\n",
    "\n",
    "# Compute the reconstruction error of the threshold set\n",
    "X_threshold_predicted = model.predict(X_threshold)\n",
    "re_threshold = np.mean(np.power(X_threshold - X_threshold_predicted, 2), axis=1)\n",
    "\n",
    "# Use the 3 sigma rule to compute the threshold\n",
    "# threshold = np.mean(re_threshold) + 3 * np.std(re_threshold)\n",
    "\n",
    "# Use the 2 sigma rule to compute the threshold\n",
    "#threshold = np.mean(re_threshold) + 2 * np.std(re_threshold)\n",
    "\n",
    "# Mad threshold\n",
    "# median_reconstruction_errors = np.median(re_threshold)\n",
    "# mad = 1.4826 * np.median(np.abs(re_threshold - median_reconstruction_errors))\n",
    "# threshold = median_reconstruction_errors + 3 * mad\n",
    "\n",
    "# IQR\n",
    "Q1 = np.percentile(re_threshold, 25)\n",
    "Q3 = np.percentile(re_threshold, 75)\n",
    "IQR = Q3 - Q1\n",
    "threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "# Assign anomaly label for each sample in X_test\n",
    "X_test_predicted = model.predict(X_test)\n",
    "re_test = np.mean(np.power(X_test - X_test_predicted, 2), axis=1)\n",
    "\n",
    "y_test_predicted = [1 if e > threshold else 0 for e in re_test]\n",
    "\n",
    "# Compute F1, precision, recall and fpr on the test dataset\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, zero_division=1)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "fpr_test = fp_test / (tn_test + fp_test)\n",
    "\n",
    "print(f\"TP: {tp_test}\")\n",
    "print(f\"FP: {fp_test}\")\n",
    "print(f\"TN: {tn_test}\")\n",
    "print(f\"FN: {fn_test}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-score: {f1_test:.4f}\")\n",
    "print(f\"FPR: {fpr_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862453f",
   "metadata": {},
   "source": [
    "<b>Evaluation Isolation Forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77478138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best feature set\n",
    "feature_set = recent_consent\n",
    "\n",
    "# Best Hyperparameter configuration\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_samples': [128],\n",
    "    'max_features': [0.6],\n",
    "    'contamination': [0.06]\n",
    "}\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# One configuration only\n",
    "all_configs = list(ParameterGrid(param_grid))\n",
    "\n",
    "X_train = df_training_copy[feature_set].copy()\n",
    "X_test = df_encoded_test_sim_copy[feature_set].copy()\n",
    "y_test = df_encoded_test_sim_copy['ActualAttack']\n",
    "\n",
    "model = IsolationForest(**all_configs[0], bootstrap=True, random_state=rng, n_jobs=-2)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Isolation Forest anomaly labels\n",
    "y_predicted_test = model.predict(X_test)\n",
    "\n",
    "# Transform the predictions from -1 to 1 and from 1 to 0\n",
    "y_predicted_test = (y_predicted_test == -1).astype(int)\n",
    "\n",
    "f1_test = f1_score(y_test_2, y_predicted_test)\n",
    "precision_test = precision_score(y_test_2, y_predicted_test, zero_division=1)\n",
    "recall_test = recall_score(y_test_2, y_predicted_test)\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test_2, y_predicted_test).ravel()\n",
    "fpr_test = fp_test / (tn_test + fp_test)\n",
    "\n",
    "print(f\"TP: {tp_test}\")\n",
    "print(f\"FP: {fp_test}\")\n",
    "print(f\"TN: {tn_test}\")\n",
    "print(f\"FN: {fn_test}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-score: {f1_test:.4f}\")\n",
    "print(f\"FPR: {fpr_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idsvenv",
   "language": "python",
   "name": "idsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
