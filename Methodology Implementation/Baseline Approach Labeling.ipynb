{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc219c4",
   "metadata": {},
   "source": [
    "<b><font size=\"5\">This Notebook provides the code for the labeling the datasets</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas dataframe\n",
    "\n",
    "df_audit = pd.read_csv('Datasets/AuditCombinedCleaned.csv')\n",
    "df_auth = pd.read_csv('Datasets/AuthCombinedCleaned.csv')\n",
    "\n",
    "# Define list of error codes \n",
    "error_codes = [50053, 50126, 50055]\n",
    "\n",
    "# Make sure that the \"createdDateTime\" is in datetime format\n",
    "df_auth['createdDateTime'] = pd.to_datetime(df_auth['createdDateTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc410639",
   "metadata": {},
   "source": [
    "<b>Baseline Labeling the sign-in dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function check whether there are more than 5 failed authentication attempts with the codes 50053, 50126 or 50055\n",
    "from the same IP address in a time window of 20 minutes \n",
    "(similar to https://github.com/Azure/Azure-Sentinel/blob/master/Solutions/Microsoft%20Entra%20ID/Analytic%20Rules/SigninPasswordSpray.yaml)\n",
    "'''\n",
    "def detect_password_spray(df_auth):\n",
    "    \n",
    "    # Define constants\n",
    "    authentication_window = pd.Timedelta(minutes=20)\n",
    "    authentication_threshold = 5\n",
    "    look_back = pd.Timedelta(days=3)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df_auth['createdDateTime'] = pd.to_datetime(df_auth['createdDateTime'])\n",
    "    df_auth['Label_Password_Spray'] = 'Normal'\n",
    "    \n",
    "    earliest_time = df_auth['createdDateTime'].min().floor('1D')\n",
    "    latest_time = df_auth['createdDateTime'].max()\n",
    "    time_range = pd.date_range(start=earliest_time, end=latest_time, freq='D')\n",
    "\n",
    "    for day in time_range:\n",
    "        current_day = day\n",
    "        end_day = day + pd.Timedelta(days=1)\n",
    "\n",
    "        # Current day events\n",
    "        daily_events = df_auth[(df_auth['createdDateTime'] >= current_day) & (df_auth['createdDateTime'] < end_day)].copy()\n",
    "        \n",
    "        # Create time windows of 20 minutes and group the events on the window and IP\n",
    "        daily_events['Window'] = daily_events['createdDateTime'].dt.floor('20min')\n",
    "        daily_grouped_failures = daily_events[daily_events['status.errorCode'].isin(error_codes)].groupby(['Window', 'callerIpAddress']).userPrincipalName.nunique().reset_index(name='failCount')\n",
    "\n",
    "        # Identify IPs that exceed the threshold within the 20 minute window\n",
    "        breached_ips = daily_grouped_failures[daily_grouped_failures['failCount'] >= authentication_threshold]\n",
    "\n",
    "        if not breached_ips.empty:\n",
    "            breached_ip_list = breached_ips['callerIpAddress'].unique().tolist()\n",
    "            print(breached_ip_list)\n",
    "            for breached_ip in breached_ip_list:\n",
    "                # Use the previous 3 days as a lookback period\n",
    "                lookback_start = current_day - look_back\n",
    "                lookback_end = end_day\n",
    "                \n",
    "                # Filter \n",
    "                lookback_df = df_auth[(df_auth['createdDateTime'] >= lookback_start) & \n",
    "                                (df_auth['createdDateTime'] < lookback_end) & \n",
    "                                (df_auth['callerIpAddress'] == breached_ip)]\n",
    "                failed_count = lookback_df[lookback_df['status.errorCode'].isin(error_codes)].shape[0]\n",
    "                success_count = lookback_df[~lookback_df['status.errorCode'].isin(error_codes)].shape[0]\n",
    "\n",
    "                if failed_count > success_count:\n",
    "                    df_auth.loc[(df_auth['createdDateTime'] >= lookback_start) &\n",
    "                           (df_auth['createdDateTime'] < lookback_end) & \n",
    "                           (df_auth['callerIpAddress'] == breached_ip), 'Label_Password_Spray'] = 'Password Spray'\n",
    "\n",
    "                    # Label any successful logins in the 20-minute window where the breach was found\n",
    "                    breach_time_blocks = breached_ips[breached_ips['callerIpAddress'] == breached_ip]['Window']\n",
    "                    for block in breach_time_blocks:\n",
    "                        block_start = block\n",
    "                        block_end = block + authentication_window\n",
    "                        df_auth.loc[(df_auth['createdDateTime'] >= block_start) & \n",
    "                               (df_auth['createdDateTime'] < block_end) & \n",
    "                               (df_auth['callerIpAddress'] == breached_ip), 'Label_Password_Spray'] = 'Password Spray'\n",
    "                else:\n",
    "                    df_auth.loc[(df_auth['createdDateTime'] >= lookback_start) & \n",
    "                           (df_auth['createdDateTime'] < lookback_end) & \n",
    "                           (df_auth['callerIpAddress'] == breached_ip), 'Label_Password_Spray'] = 'Normal'\n",
    "\n",
    "    return df_auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function check whether a sign-in burst from multiple locations and from the same account happened in a 1 hour time window.\n",
    "In comaparison with the original query, the code checks for bursts with sign-in failed attempts (i.e. code is 50053, 50126, or 50055)\n",
    "(similar to https://github.com/Azure/Azure-Sentinel/blob/master/Solutions/Microsoft%20Entra%20ID/Analytic%20Rules/Sign-in%20Burst%20from%20Multiple%20Locations.yaml)\n",
    "'''\n",
    "\n",
    "def detect_signin_burst(df_auth):\n",
    "    \n",
    "    # Instead of monitoring only one app, monitor all the apps identified in the 'appDisplayName' column\n",
    "    list_apps = set(df_auth['appDisplayName'])\n",
    "    \n",
    "    # Discretize the 'createdDateTime' column to 1-hour bins\n",
    "    df_auth['Window'] = pd.to_datetime(df_auth['createdDateTime']).dt.floor('1H')\n",
    "    \n",
    "    # Initializing the \"Label_Burst_1_hour\"\n",
    "    df_auth['Label_Burst_1_hour'] = 'Normal'\n",
    "    \n",
    "    # Filtering failure codes and relevant apps\n",
    "    failures_df = df_auth[df_auth['status.errorCode'].isin(error_codes) & df_auth['appDisplayName'].isin(list_apps)]\n",
    "\n",
    "    # Group by 'userPrincipalName', 'appDisplayName' and 'Window'\n",
    "    grouping_list = ['userPrincipalName', 'Window', 'appDisplayName']\n",
    "    grouping = failures_df.groupby(grouping_list)\n",
    "    locations_count_group = grouping['location.countryOrRegion'].nunique().reset_index(name='Unique_Locations')\n",
    "    \n",
    "    # Filter events in the grouping with more than one location\n",
    "    suspicious_events = locations_count_group[locations_count_group['Unique_Locations'] > 1]\n",
    "    \n",
    "    # Updating the \"Label_Burst_1_hour\" by merging with information from \n",
    "    if not suspicious_events.empty:\n",
    "        df_auth = pd.merge(df_auth, suspicious_events[grouping_list], \n",
    "                      on=grouping_list, \n",
    "                      how='left', indicator=True)\n",
    "        \n",
    "        df_auth.loc[df_auth['_merge'] == 'both', 'Label_Burst_1_hour'] = 'Signin Burst'\n",
    "        df_auth.drop(columns=['_merge'], inplace=True)\n",
    "    # Drop the 'Window' column\n",
    "    df_auth.drop(columns=['Window'], inplace=True)\n",
    "    \n",
    "    return df_auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Detect password cracking attempts in a day using a threshold of more than 3 different location used to attack and account \n",
    "and more than 30 failed sign-ins. \n",
    "(similar to https://github.com/Azure/Azure-Sentinel/blob/master/Solutions/Microsoft%20Entra%20ID/Analytic%20Rules/DistribPassCrackAttempt.yaml)\n",
    "'''\n",
    "\n",
    "def detect_password_cracking(df_auth):\n",
    "\n",
    "    # Threshold number of sign-ins to monitor\n",
    "    signin_threshold = 30  \n",
    "    # Threshold number of locations to monitor\n",
    "    location_threshold = 3  \n",
    "    \n",
    "    # Create a new column for labels\n",
    "    df_auth['Label_Password_Cracking'] = 'Normal'\n",
    "    \n",
    "    # Discretize the 'createdDateTime'\n",
    "    df_auth['Day'] = df_auth['createdDateTime'].dt.floor('1D')\n",
    "    unique_days = df_auth['Day'].unique()\n",
    "\n",
    "    for day in unique_days:\n",
    "        # Include only the data for one day\n",
    "        daily_df = df_auth[df_auth['Day'] == day]\n",
    "\n",
    "        # Group by user and aggregate relevant metrics\n",
    "        grouping = daily_df[daily_df['status.errorCode'].isin(error_codes)].groupby('userPrincipalName').agg({\n",
    "            'location.countryOrRegion': 'nunique',\n",
    "            'status.errorCode': 'count'\n",
    "        }).reset_index()\n",
    "        grouping.columns = ['userPrincipalName', 'Location_Count', 'SigninCount']\n",
    "\n",
    "        # Identify unexpected behavior from users using the thresholds\n",
    "        suspicious_users = grouping[(grouping['SigninCount'] > signin_threshold) & \n",
    "                                    (grouping['Location_Count'] >= location_threshold)]\n",
    "        if not suspicious_users.empty:\n",
    "            for index, row in suspicious_users.iterrows():\n",
    "                user = row['userPrincipalName']\n",
    "                df_auth.loc[(df_auth['Day'] == day) & \n",
    "                            (df_auth['userPrincipalName'] == user) & \n",
    "                            (df_auth['status.errorCode'].isin(error_codes)), 'Label_Password_Cracking'] = 'Password Cracking'\n",
    "    \n",
    "    # Drop the 'Day' columns\n",
    "    df_auth.drop(columns=['Day'], inplace=True)\n",
    "\n",
    "    return df_auth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9257b54",
   "metadata": {},
   "source": [
    "<b>Baseline Labeling the audit dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03964dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Monitor the audit logs for applications requesting consent for a scope of permissions that includes \"RoleManagement.ReadWrite.Directory\"\n",
    "(similar to https://github.com/Azure/Azure-Sentinel/blob/master/Solutions/Microsoft%20Entra%20ID/Analytic%20Rules/AzureADRoleManagementPermissionGrant.yaml)\n",
    "'''\n",
    "\n",
    "def detect_role_management(df_audit):\n",
    "\n",
    "    df_audit['Label_Role_Management'] = \"Normal\"\n",
    "    \n",
    "    # Monitor the activities containing the approle and delegated permission scopes\n",
    "    df_audit_aux = df_audit[df_audit['Activity'].isin([\"Add delegated permission grant\", \n",
    "                                                       \"Add app role assignment to service principal\"])]\n",
    "    \n",
    "    # Check the scope of the app for the permission \"RoleManagement.ReadWrite.Directory\" (both approle and delegated)\n",
    "    df_audit_aux = df_audit_aux[\n",
    "        (df_audit_aux['Target1ModifiedProperty2NewValue'].str.contains(\"RoleManagement.ReadWrite.Directory\")) |\n",
    "        (df_audit_aux['Target1ModifiedProperty1NewValue'].str.contains(\"RoleManagement.ReadWrite.Directory\"))\n",
    "    ]\n",
    "\n",
    "    # Label rows accordingly in the main dataframe\n",
    "    df_audit.loc[df_audit_aux.index, 'Label_Role_Management'] = \"Malicious_consent\"\n",
    "\n",
    "    # Propagate label to all rows with same correlationId\n",
    "    correlation_ids = df_audit_aux['CorrelationId'].unique()\n",
    "    df_audit.loc[df['CorrelationId'].isin(correlation_ids), 'Label_Role_Management'] = \"Malicious_consent\"\n",
    "\n",
    "    return df_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Monitor the audit logs for evidence of illicit consent grant that use the same permission scope as \"o365-attack-toolkit\" \n",
    "(o365-attack-toolkit: https://github.com/mdsecactivebreach/o365-attack-toolkit)\n",
    "(similar to https://github.com/Azure/Azure-Sentinel/blob/master/Solutions/Microsoft%20Entra%20ID/Analytic%20Rules/MaliciousOAuthApp_O365AttackToolkit.yaml)\n",
    "'''\n",
    "def detect_role_management(df_audit):\n",
    "    \n",
    "    permissions_o365 = [\"Contacts.Read\", \"User.Read\", \"Mail.Read\", \"Notes.Read.All\", \n",
    "                        \"MmailboxSettings.ReadWrite\", \"Files.ReadWrite.All\", \"Mail.Send\", \n",
    "                        \"Files.Read\", \"Files.Read.All\"]\n",
    "    legitimate_apps = [\"DocuSign NA1\", \"Zoom Video Communications\", \"Zoom OneDrive\", \"Windows Defender Security Intelligence\", \n",
    "                       \"Amazon Alexa Connect\", \"Barracuda Networks\", \"Favro\", \"ClassFlow Prod OneDrive Personal\"]\n",
    "    \n",
    "    # The least number of risky permissions that in the scope of an app, in order to be considered legitimate\n",
    "    threshold = 3\n",
    "    \n",
    "    df_audit['Label_o365'] = 'Normal'\n",
    "    df_audit_aux = df_audit[df_audit['Activity'] == 'Consent to application']\n",
    "    df_audit_aux = df_audit_aux[~df_audit_aux['Target1DisplayName'].isin(legitimate_apps)]\n",
    "\n",
    "    # Identify relevant \"Add delegated permission grant\" activities\n",
    "    consent_flow = df_audit[df_audit['CorrelationId'].isin(df_audit_aux['CorrelationId'].unique())]\n",
    "    deleg_permissions = consent_flow[consent_flow['Activity'] == 'Add delegated permission grant']\n",
    "\n",
    "    # The consent type is not \"AllPrincipals\" and RiskyPermissions in scope > threshold\n",
    "    for index, row in deleg_permissions.iterrows():\n",
    "        if row['Target1ModifiedProperty2NewValue'] != \"AllPrincipals\":\n",
    "            permissions = row['Target1ModifiedProperty1NewValue'].split(' ')\n",
    "            matching_perms = [permission for permission in permissions if permission in permissions_o365]\n",
    "            if len(matching_perms) > threshold:\n",
    "                df_audit.loc[df_audit['CorrelationId'] == row['CorrelationId'], 'Label_o365'] = 'Malicious_consent'\n",
    "\n",
    "    # Identify \"Add service principal\" activities within 14 days of labeled \"Consent to application\"\n",
    "    service_principal_df = df_audit[df_audit['Activity'] == 'Add service principal']\n",
    "    labeled_consent = df_audit[df_audit['Label_o365'] == 'Malicious_consent']\n",
    "    for index, row in service_principal_df.iterrows():\n",
    "        time_range = pd.date_range(end=row['Date (UTC)'], periods=14, freq='D')\n",
    "        related = labeled_consent[labeled_consent['Date (UTC)'].isin(time_range)]\n",
    "        if not related.empty:\n",
    "            df_audit.loc[index, 'Label_o365'] = 'Malicious_consent'\n",
    "\n",
    "    return df_audit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idsvenv",
   "language": "python",
   "name": "idsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
